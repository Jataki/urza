>>>> main.py
from agent import StrategistAgent
import argparse

def main():
    parser = argparse.ArgumentParser(description="Card Game Strategist Agent")
    parser.add_argument("--query", type=str, help="User query")
    args = parser.parse_args()
    
    agent = StrategistAgent()
    
    if args.query:
        # Process a single query
        response = agent.process_query(args.query)
        print(f"Response: {response['answer']}")
    else:
        # Interactive mode
        print("Magic the Gathering agent (type 'exit' to quit, 'reset' to clear history)")
        while True:
            query = input("\nUser: ")
            if query.lower() == "exit":
                break
            if query.lower() == "reset":
                agent.reset_conversation()
                print("Conversation history reset.")
                continue
                
            response = agent.process_query(query)
            print(f"\nStrategist: {response['answer']}")

if __name__ == "__main__":
    main()

>>>> requirements.txt
aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
annotated-types==0.7.0
anyio==4.8.0
asgiref==3.8.1
attrs==25.3.0
backoff==2.2.1
bcrypt==4.3.0
build==1.2.2.post1
cachetools==5.5.2
certifi==2025.1.31
charset-normalizer==3.4.1
chroma-hnswlib==0.7.6
chromadb==0.6.3
click==8.1.8
coloredlogs==15.0.1
dataclasses-json==0.6.7
Deprecated==1.2.18
distro==1.9.0
durationpy==0.9
fastapi==0.115.11
filelock==3.18.0
filetype==1.2.0
flatbuffers==25.2.10
frozenlist==1.5.0
fsspec==2025.3.0
google-ai-generativelanguage==0.6.16
google-api-core==2.24.2
google-auth==2.38.0
google-genai==1.5.0
googleapis-common-protos==1.69.1
greenlet==3.1.1
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.14.0
httpcore==1.0.7
httptools==0.6.4
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.29.3
humanfriendly==10.0
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
jsonpatch==1.33
jsonpointer==3.0.0
kubernetes==32.0.1
langchain==0.3.20
langchain-community==0.3.19
langchain-core==0.3.45
langchain-google-genai==2.1.0
langchain-text-splitters==0.3.6
langfuse==2.59.7
langsmith==0.3.15
markdown-it-py==3.0.0
marshmallow==3.26.1
mdurl==0.1.2
mmh3==5.1.0
monotonic==1.6
mpmath==1.3.0
multidict==6.1.0
mypy-extensions==1.0.0
numpy==2.2.3
oauthlib==3.2.2
onnxruntime==1.21.0
opentelemetry-api==1.31.0
opentelemetry-exporter-otlp-proto-common==1.31.0
opentelemetry-exporter-otlp-proto-grpc==1.31.0
opentelemetry-instrumentation==0.52b0
opentelemetry-instrumentation-asgi==0.52b0
opentelemetry-instrumentation-fastapi==0.52b0
opentelemetry-proto==1.31.0
opentelemetry-sdk==1.31.0
opentelemetry-semantic-conventions==0.52b0
opentelemetry-util-http==0.52b0
orjson==3.10.15
overrides==7.7.0
packaging==24.2
posthog==3.20.0
propcache==0.3.0
proto-plus==1.26.1
protobuf==5.29.3
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.6
pydantic-settings==2.8.1
pydantic_core==2.27.2
Pygments==2.19.1
pypdf==5.3.1
PyPika==0.48.9
pyproject_hooks==1.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
PyYAML==6.0.2
requests==2.32.3
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
rich==13.9.4
rsa==4.9
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.39
starlette==0.46.1
sympy==1.13.3
tenacity==9.0.0
tokenizers==0.21.1
tqdm==4.67.1
typer==0.15.2
typing-inspect==0.9.0
typing_extensions==4.12.2
urllib3==2.3.0
uvicorn==0.34.0
uvloop==0.21.0
watchfiles==1.0.4
websocket-client==1.8.0
websockets==14.2
wrapt==1.17.2
yarl==1.18.3
zipp==3.21.0
zstandard==0.23.0


>>>> agent.py
from google import genai
from retriever import KnowledgeBaseRetriever
from prompts import get_mtg_strategist_prompt, get_context_prompt
from config import GOOGLE_API_KEY, MODEL_NAME, TEMPERATURE
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_message_histories import ChatMessageHistory

class StrategistAgent:
    def __init__(self):        
        # Initialize LLM
        self.llm = ChatGoogleGenerativeAI(
            model=MODEL_NAME, 
            temperature=TEMPERATURE, 
            google_api_key=GOOGLE_API_KEY
        )
        
        # Initialize retriever
        self.base_retriever = KnowledgeBaseRetriever().initialize().get_retriever()
        
        # Build the chain
        self.chain = self._build_chain()

        # Statefully manage chat history
        self.store = {}

        
    def _build_chain(self):
        """Build the history-aware retrieval chain"""
        # Create history-aware retriever
        history_aware_retriever = create_history_aware_retriever(
            self.llm, 
            self.base_retriever, 
            get_context_prompt()
        )
        
        # Create document chain
        question_answer_chain = create_stuff_documents_chain(
            self.llm, 
            get_mtg_strategist_prompt()
        )
        
        # Create retrieval chain
        rag_chain = create_retrieval_chain(
            history_aware_retriever, 
            question_answer_chain
        )
        
        return RunnableWithMessageHistory(
            rag_chain,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
    
    def process_query(self, query, session_id="default_session"):
        """Process a user query using the agent chain"""
        response = self.chain.invoke(
            {"input": query},
            {"configurable": {"session_id": session_id}}
        )
        
        return response
    
    def reset_conversation(self):
        """Reset the conversation history"""
        self.chat_history.clear()

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]


>>>> prompts.py
from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder

def get_context_prompt():
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    return ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

def get_mtg_strategist_prompt():
    template = """You are a Magic: The Gathering Strategy Expert who provides deck-building advice.

IMPORTANT CONSTRAINTS:
- Only provide advice for Magic: The Gathering (MtG). Politely decline queries about other games.
- When suggesting cards, NEVER recommend specific card names. Instead, describe card parameters (color, mana cost, keywords, types, subtypes, etc).
- Always consider the specified format and its ban list when applicable. If no format is mentioned, ask for clarification.
- Decline queries unrelated to MtG with a brief explanation.

Question: {input}

Context (MtG Rules/Meta Information):
{context}

Respond with:
1. Strategic Analysis: Identify key synergies, mechanics, and strategic elements relevant to the query
2. Archetype Guidance: Suggest potential deck archetypes that align with the request
3. Parameter-Based Card Suggestions: Describe card characteristics to look for (NOT specific card names)
4. Format Considerations: Address format-specific strategies and restrictions if a format is specified
5. Mana Curve & Resource Management: Provide guidance on optimal mana distribution and resource utilization

Think step-by-step to provide comprehensive yet targeted strategic advice.
"""
    return ChatPromptTemplate.from_messages([
        ("system", template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])


>>>> config.py
import os
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables from .env file
env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path)

# API Keys
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY environment variable is not set")

# Agent Configuration
MODEL_NAME = os.environ.get("MODEL_NAME", "gemini-2.0-flash")
TEMPERATURE = float(os.environ.get("TEMPERATURE", "0.8"))
KNOWLEDGE_BASE_DIR = os.environ.get("KNOWLEDGE_BASE_DIR", "knowledge_base")

>>>> retriever.py
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import GOOGLE_API_KEY, KNOWLEDGE_BASE_DIR
import os

class KnowledgeBaseRetriever:
    def __init__(self, embedding_model="models/text-embedding-004"):
        self.knowledge_dir = KNOWLEDGE_BASE_DIR
        self.embedding_model = embedding_model
        self.vector_store = None
        self.embeddings = None
        
    def initialize(self):
        """Initialize the vector store with documents from the knowledge base"""
        # Initialize embeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=self.embedding_model, 
            google_api_key=GOOGLE_API_KEY
        )
        
        # Load documents
        loader = DirectoryLoader(self.knowledge_dir, glob="**/*.pdf", loader_cls=PyPDFLoader)
        documents = loader.load()
        
        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
        splits = text_splitter.split_documents(documents)
        
        # Create vector store
        self.vector_store = Chroma.from_documents(documents=splits, embedding=self.embeddings)
        
        return self
    
    def get_retriever(self, k=5):
        """Return the retriever with top k results"""
        if not self.vector_store:
            self.initialize()
        return self.vector_store.as_retriever(search_kwargs={"k": k})

